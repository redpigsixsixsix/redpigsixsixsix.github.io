---
title:  "利用LSTM方法预测BTC价格"
layout: post
---

# 什么是LSTM
循环神经网络（Recurrent Neural Network，RNN）是一种用于处理序列数据的神经网络。相比一般的神经网络来说，他能够处理序列变化的数据。比如某个单词的意思会因为上文提到的内容不同而有不同的含义，RNN就能够很好地解决这类问题。
LSTM是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。LSTM有三个门：更新门、遗忘门和输出门。更新和忘记门决定是否更新单元的每个元素。    

LSTM内部有三个阶段:
1. 忘记阶段：这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会"忘记不重要的，记住重要的"。
2. 选择记忆阶段：这个阶段的输入进行有选择性的进行记忆，主要是对输入进行选择。
3. 输出阶段：这个阶段决定哪些将会当成当前状态的输出。

![image.png](https://i.loli.net/2021/10/26/oUBEwGvMWOflgDP.png)

他的核心思想就是细胞状态，水平线在图上方贯穿运行，细胞状态类似于传送带，直接在整个链上运行，只有一些少量的信息交互。因此如果想让信息保持不变的话会很容易。

![image](https://user-images.githubusercontent.com/67320649/138860697-9c26a0cf-e57c-4a69-aacb-9fb1e469909d.png)

LSTM有通过精心设计的称作门的结构来去除或者增加信息到细胞的能力。门是一种让信息选择式通过的方法，包含一个`sigmoid`神经网络层和一个按位的乘法操作。  

![image](https://user-images.githubusercontent.com/67320649/138861434-d8fbf5b1-3789-4356-b8ae-0b00508adff9.png)

`sigmod`层输出0-1之间的数值，描述每个部分有多少量可以通过。0表示“不允许任何量通过”，1表示“允许任意量通过”。  
LSTM有三个门，保护和控制细胞状态。  

## LTSM的细节部分

LSTM中的第一步表示的是我们会从细胞状态中丢掉什么信息，这个决定通过**忘记门层**完成。这个门会读取 ![](http://latex.codecogs.com/gif.latex?h_{t-1}) 和 ![](http://latex.codecogs.com/gif.latex?x_{t})。  
输出一个0-1之间的数值，给 ![](http://latex.codecogs.com/gif.latex?C_{t-1}) 中的数字。

让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。

![image](https://user-images.githubusercontent.com/67320649/138863033-a52da9f6-5520-4127-a51e-c815fa98f6ac.png)

下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，`sigmoid`层称"输入门层"决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。  
在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。

![image](https://user-images.githubusercontent.com/67320649/138863761-7f5d95df-c7b6-4f43-ab92-5ac1b506dfc0.png)

现在是更新旧细胞状态的时间了，![](http://latex.codecogs.com/gif.latex?C_{t-1}) 更新为 ![](http://latex.codecogs.com/gif.latex?C_{t}) 。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。  
把旧状态和 ![](http://latex.codecogs.com/gif.latex?f_{t}) 相乘，丢弃掉我们确定需要丢弃的信息，接着加上 ![](http://latex.codecogs.com/gif.latex?i_{t}*C_{t})。这个就是新的候选值，根据我们决定更新每个状态的程序进行变化。  
在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。

![image](https://user-images.githubusercontent.com/67320649/138864408-a0e2c7ce-4abb-4448-a701-7a3c93decae9.png)

最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个`sigmoid`层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过`tanh`进行处理（得到一个在-1到1之间的值）并将它和`sigmoid`门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。

在语言模型的例子中，因为他就看到了一个**代词**，可能需要输出与一个**动词**相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。

![image](https://user-images.githubusercontent.com/67320649/138864950-4f4783e9-436a-4021-86a1-ec0620f6beb5.png)

# 使用LSTM对价格进行预测

这一部分研究一下怎么通过keras框架对BTC未来的价格进行预测。

## 构造LSTM模型

选择Relu作为激活函数

![image](https://user-images.githubusercontent.com/67320649/138993734-d8f3736d-01eb-413d-b7fb-0e7a1cf3606a.png)

Relu函数的优势有以下几点:  
- 没有饱和区，不存在梯度消失问题。
- 没有复杂的指数运算，计算简单、效率提高。
- 实际收敛速度较快，比`Sigmoid/tanh`快很多。
- 比`Sigmoid`更符合生物学神经激活机制。

Relu的缺点:  
- 在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度，流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。
- 如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。

如果在训练的时候解决不了梯度爆炸的问题最好还是换回`tanh`激活函数

![image](https://user-images.githubusercontent.com/67320649/138994184-d0817d98-4b22-4088-a9ae-72009042645a.png)

相对于tanh，relu收敛更快，准确率更高。  
但是我在实际测试的过程当中发现relu的效果更加好，模型收敛的更快一些。  

```
model = Sequential()
model.add(LSTM(units=128, activation='relu', dropout=0.2, input_shape=(21,1))) # Input layer
model.add(Dense(units=1)) # Output layer
model.compile(optimizer='adam', loss='mae') #rmse?
model.fit(X_train, y_train, epochs=50, batch_size=7)
```
最后的`model.fit`表示其中的隐藏层有50个神经元，输出层有1个神经元。  
batch_size表示的是一次要喂多少个样本给神经网络，让神经网络来计算迭代时的梯度。比如，batch_size=500的时候表示一次喂500个样本数据给神经网络，batch_size=1的时候表示一次喂1个样本给神经网络。  
在同等的计算量之下，使用整个样本集的收敛速度远慢于使用少量样本的情况。换句话说，要想收敛到同一个最优点，使用整个样本集时，虽然迭代次数少，但是每次迭代的时间长，耗费的总时间大于使用少量样本多次迭代的情况。  
那么是不是样本越少收敛越快呢？理论上是这样的，但是如果你用GPU并行来计算的话，耗时会根据GPU的线程数计算。  
实际测试的过程中，batch_size的大小反而是越小效果越好（最后的loss越小）。  

## 获得训练集和测试集

首先我们要将现有的数据分为两类，一类是训练集，一类是测试集。  
数据是从[这个网址](https://www.kaggle.com/mczielinski/bitcoin-historical-data)上获取的，获取的数据内容如下:  
``` 
Timestamp,Open,High,Low,Close,Volume_(BTC),Volume_(Currency),Weighted_Price
1325317920,4.39,4.39,4.39,4.39,0.45558087,2.0000000193,4.39
```
用pandas库来处理csv文件，parse_dates参数表示对csv文件中日期序列的处理方式：
- 默认是False，原样加载，不解析日期时间
- 可以为True，尝试解析日期索引
- 可以为数字或者`names`的列表，解析指定的列为时间序列
- 可以为以列表为元素的列表，解析每个子列表中的字段组合为时间序列
- 可以为值为列表的字典，解析每个列表中的字段组合为时间序列，并命名为字典中对应的键值

date_parser是一个制定解析日期的函数
```python3
def dateparse (time_in_secs):    
    return pytz.utc.localize(datetime.datetime.fromtimestamp(float(time_in_secs)))
```
当使用过date_parser函数处理过的数据，可以通过时间筛选数据，例如：  

![image](https://user-images.githubusercontent.com/67320649/139232205-69d3735f-de5d-4713-8961-aeb8db66b21a.png)

上面的图片表示筛选出2020年之后的收盘价数据和2020年1月2日之后的收盘价数据。   
然后因为数据里面有Nan的数据，所以用0将这些数据填满。  
```
df['Volume_(BTC)'].fillna(value=0, inplace=True)
df['Volume_(Currency)'].fillna(value=0, inplace=True)
df['Weighted_Price'].fillna(value=0, inplace=True)
```
随后因为价格是主要用来学习的参数，所以用forwardfill进行填充，forwardfill就是用上一层的数据填充这一层的数据。  
```
df['Open'].fillna(method='ffill', inplace=True)
df['High'].fillna(method='ffill', inplace=True)
df['Low'].fillna(method='ffill', inplace=True)
df['Close'].fillna(method='ffill', inplace=True)
```
接下来用groupby函数按照每天的收盘价，算出收盘价格的均价。





